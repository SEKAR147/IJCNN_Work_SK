{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SEKAR147/IJCNN_Work_SK/blob/main/Gemma_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MrMGb75HY20"
      },
      "outputs": [],
      "source": [
        "!pip install -q huggingface_hub>=0.25.0 transformers>=4.45.0\n",
        "!pip install -U transformers accelerate\n",
        "!pip install -q accelerate bitsandbytes timm\n",
        "import torch\n",
        "from PIL import Image\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "from transformers import (\n",
        "    AutoProcessor,\n",
        "    AutoModelForZeroShotObjectDetection,\n",
        "    Blip2Processor,\n",
        "    Blip2ForConditionalGeneration,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqTDYKmBHeFB"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "dino_id = \"IDEA-Research/grounding-dino-tiny\"\n",
        "dino_processor = AutoProcessor.from_pretrained(dino_id)\n",
        "dino_model = AutoModelForZeroShotObjectDetection.from_pretrained(dino_id).to(device)\n",
        "\n",
        "\n",
        "pali_id = \"google/paligemma2-3b-mix-224\"\n",
        "pali_processor = AutoProcessor.from_pretrained(pali_id)\n",
        "pali_model = AutoModelForImageTextToText.from_pretrained(\n",
        "    pali_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RK-6knP-ILeO"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets\n",
        "from datasets import load_dataset\n",
        "from itertools import islice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zH762T_YiMN"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"lemmatizer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsrhIU2vHg4S"
      },
      "outputs": [],
      "source": [
        "def run_pipeline_streaming(image, text_prompt, question_id, image_id, base_drive_path):\n",
        "    question_dir = os.path.join(base_drive_path, question_id)\n",
        "    crops_dir = os.path.join(question_dir, \"crops\")\n",
        "    os.makedirs(crops_dir, exist_ok=True)\n",
        "\n",
        "    image = image.convert(\"RGB\")\n",
        "    image.save(os.path.join(question_dir, \"original.jpg\"))\n",
        "\n",
        "    COLOR_SET = {\n",
        "        \"red\", \"blue\", \"green\", \"yellow\", \"black\", \"white\", \"silver\", \"gold\", \"orange\",\n",
        "        \"pink\", \"purple\", \"brown\", \"gray\", \"grey\", \"tan\", \"turquoise\", \"beige\", \"maroon\",\n",
        "        \"navy\", \"teal\", \"azure\", \"bronze\", \"copper\", \"lavender\", \"violet\", \"blonde\",\n",
        "        \"light\", \"dark\", \"bright\", \"pale\", \"colorful\", \"multicolored\"\n",
        "    }\n",
        "\n",
        "    MATERIAL_SET = {\n",
        "        \"wood\", \"metal\", \"plastic\", \"glass\", \"brick\", \"denim\", \"leather\", \"cloth\", \"stone\",\n",
        "        \"concrete\", \"paper\", \"rubber\", \"wool\", \"silk\", \"velvet\", \"fabric\", \"cotton\", \"nylon\",\n",
        "        \"steel\", \"aluminum\", \"iron\", \"chrome\", \"porcelain\", \"ceramic\", \"tile\", \"marble\",\n",
        "        \"asphalt\", \"cardboard\", \"wicker\", \"vinyl\", \"suede\", \"fleece\"\n",
        "    }\n",
        "\n",
        "    TEXTURE_SET = {\n",
        "        \"smooth\", \"rough\", \"shiny\", \"metallic\", \"soft\", \"hard\", \"fuzzy\", \"clear\", \"dull\",\n",
        "        \"matte\", \"wet\", \"dry\", \"painted\", \"polished\", \"glossy\", \"checkered\", \"striped\",\n",
        "        \"dotted\", \"patterned\", \"cracked\", \"dirty\", \"clean\", \"worn\", \"new\", \"old\", \"fuzzy\",\n",
        "        \"hairy\", \"scratched\", \"wrinkled\", \"rusted\", \"transparent\", \"opaque\"\n",
        "    }\n",
        "\n",
        "    SHAPE_SET = {\n",
        "        \"round\", \"square\", \"rectangular\", \"triangular\", \"oval\", \"flat\", \"curved\", \"pointed\",\n",
        "        \"thick\", \"thin\", \"wide\", \"narrow\", \"large\", \"small\", \"tiny\", \"huge\", \"long\", \"short\",\n",
        "        \"tall\", \"curvy\", \"straight\", \"bent\", \"circular\", \"spherical\", \"conical\", \"cylindrical\"\n",
        "    }\n",
        "\n",
        "    CLOTHING_SET = {\n",
        "        \"shirt\", \"pants\", \"hat\", \"glasses\", \"jacket\", \"jeans\", \"dress\", \"shorts\", \"shoes\",\n",
        "        \"t-shirt\", \"sweater\", \"suit\", \"tie\", \"skirt\", \"boots\", \"sneakers\", \"socks\", \"gloves\",\n",
        "        \"scarf\", \"belt\", \"cap\", \"helmet\", \"uniform\", \"vest\", \"coat\", \"hoodie\"\n",
        "    }\n",
        "\n",
        "    # --- PHASE 1: GROUNDING DINO ---\n",
        "    inputs = dino_processor(images=image, text=text_prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = dino_model(**inputs)\n",
        "\n",
        "    results = dino_processor.post_process_grounded_object_detection(\n",
        "        outputs, inputs.input_ids,\n",
        "        threshold=0.35, text_threshold=0.25,\n",
        "        target_sizes=[image.size[::-1]]\n",
        "    )[0]\n",
        "\n",
        "    final_data = []\n",
        "\n",
        "    # --- PHASE 2 & 3: OBJECT LOOP ---\n",
        "    for i, (score, label, box) in enumerate(zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"])):\n",
        "        box = [round(b, 2) for b in box.tolist()]\n",
        "        crop = image.crop((box[0], box[1], box[2], box[3]))\n",
        "\n",
        "        if crop.size[0] < 40 or crop.size[1] < 40:\n",
        "            continue\n",
        "\n",
        "        crop_name = f\"obj_{i}_{label}.jpg\"\n",
        "        crop.save(os.path.join(crops_dir, crop_name))\n",
        "\n",
        "        extracted_attributes = []\n",
        "        actions = []\n",
        "        obj_id = f\"gqa_2026_{question_id}_obj_{i}\"\n",
        "\n",
        "        # --- PHASE 2: PALIGEMMA ---\n",
        "        if label.lower() == \"person\":\n",
        "            instruction = \"<image> answer en what is this person wearing and what are they doing?\\n\"\n",
        "        else:\n",
        "            instruction = f\"<image> answer en what is the color, material, and texture of this {label}?\\n\"\n",
        "\n",
        "        inputs = pali_processor(text=instruction, images=crop, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = pali_model.generate(**inputs, max_new_tokens=60, do_sample = False, repetition_penalty = 1.1)\n",
        "\n",
        "            prompt_length = inputs.input_ids.shape[1]\n",
        "            new_tokens = output[0][prompt_length:]\n",
        "            desc = pali_processor.decode(new_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "        # --- PHASE 3: SPACY ---\n",
        "\n",
        "        doc = nlp(desc.lower())\n",
        "        categorized_attr = {\n",
        "            \"color\": [],\n",
        "            \"material\": [],\n",
        "            \"texture\": [],\n",
        "            \"shape\": [],\n",
        "            \"action\": [],\n",
        "            \"clothing\": []\n",
        "        }\n",
        "\n",
        "        for token in doc:\n",
        "            t_text = token.text\n",
        "            # 1. Action/Pose: Capture Present Participle verbs (-ing)\n",
        "            if token.pos_ == \"VERB\" and token.tag_ == \"VBG\":\n",
        "                if t_text not in [\"is\", \"being\", \"having\", \"pose\", \"wearing\"]:\n",
        "                    categorized_attr[\"action\"].append(t_text)\n",
        "\n",
        "            # 2. Categorize based on GQA Sets\n",
        "            if t_text in COLOR_SET:\n",
        "                categorized_attr[\"color\"].append(t_text)\n",
        "            if t_text in MATERIAL_SET:\n",
        "                categorized_attr[\"material\"].append(t_text)\n",
        "            if t_text in TEXTURE_SET:\n",
        "                categorized_attr[\"texture\"].append(t_text)\n",
        "            if t_text in SHAPE_SET:\n",
        "                categorized_attr[\"shape\"].append(t_text)\n",
        "            if t_text in CLOTHING_SET:\n",
        "                categorized_attr[\"clothing\"].append(t_text)\n",
        "\n",
        "        categorized_attr = {k: list(set(v)) for k, v in categorized_attr.items()}\n",
        "\n",
        "        final_data.append({\n",
        "            \"question_id\": question_id,\n",
        "            \"img_id\": image_id,\n",
        "            \"obj_id\": obj_id,\n",
        "            \"box\": box,\n",
        "            \"object_type\": label,\n",
        "            \"attributes\": categorized_attr,\n",
        "            \"crop_path\": os.path.join(\"crops\", crop_name),\n",
        "            \"raw\": desc\n",
        "        })\n",
        "\n",
        "    with open(os.path.join(question_dir, \"data.json\"), \"w\") as f:\n",
        "        json.dump(final_data, f, indent=4)\n",
        "\n",
        "    del results, inputs, outputs\n",
        "    return final_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NylmSHX6xXeE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "!fusermount -u /content/drive\n",
        "\n",
        "!rm -rf /content/drive\n",
        "\n",
        "!mkdir /content/drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "drive_path = \"/content/drive/MyDrive/GQA_Research_2026_6\"\n",
        "os.makedirs(drive_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PHcu0EQyIMMK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "import os\n",
        "from itertools import islice\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"lmms-lab/GQA\", \"val_balanced_instructions\", streaming=True, split=\"val\")\n",
        "\n",
        "image_dataset = load_dataset(\"lmms-lab/GQA\", \"val_balanced_images\", streaming=True, split=\"val\")\n",
        "\n",
        "print(\"Building image lookup table...\")\n",
        "image_lookup = {}\n",
        "for img_ex in islice(image_dataset, 5000):\n",
        "    image_lookup[img_ex['id']] = img_ex['image']\n",
        "\n",
        "all_final_results = []\n",
        "num_questions = 1000\n",
        "prompt = \"person . clothing . bag . chair . table . house . boat . door . animal . car . sign . bottle . cup . food . plate .\"\n",
        "\n",
        "print(f\"Starting stream for {num_questions} questions...\")\n",
        "\n",
        "for i, example in enumerate(islice(dataset, num_questions)):\n",
        "    q_id = example['id']\n",
        "    img_id = example['imageId']\n",
        "\n",
        "    if img_id not in image_lookup:\n",
        "        print(f\"Skipping Q {q_id}: Image {img_id} not found in cache.\")\n",
        "        continue\n",
        "\n",
        "    img = image_lookup[img_id]\n",
        "\n",
        "    try:\n",
        "        image_results = run_pipeline_streaming(img, prompt,question_id = q_id, image_id=img_id, base_drive_path= drive_path)\n",
        "        all_final_results.extend(image_results)\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping image {i} due to error: {e}\")\n",
        "\n",
        "    if i % 50 == 0 and i > 0:\n",
        "        checkpoint_file = os.path.join(drive_path, \"results_checkpoint.json\")\n",
        "        with open(checkpoint_file, \"w\") as f:\n",
        "            json.dump(all_final_results, f)\n",
        "        print(f\"Checkpoint saved at image {i}\")\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "final_file = os.path.join(drive_path, \"final_gqa_results.json\")\n",
        "with open(final_file, \"w\") as f:\n",
        "    json.dump(all_final_results, f)\n",
        "\n",
        "print(f\"Done! All results are saved in {drive_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eg1iG54qldVD"
      },
      "outputs": [],
      "source": [
        "def check_clothing_match(p_attr, p_bbox, gt_objs, img_id):\n",
        "    \"\"\"\n",
        "    Checks if a predicted clothing attribute (e.g., 'shirt') matches\n",
        "    a separate 'shirt' object in the GQA ground truth for that image.\n",
        "    \"\"\"\n",
        "    for g_id, g_obj in gt_objs.items():\n",
        "        g_name = g_obj['name'].lower()\n",
        "\n",
        "        if p_attr.lower() in g_name or g_name in p_attr.lower():\n",
        "\n",
        "            g_bbox = [g_obj['x'], g_obj['y'], g_obj['x'] + g_obj['w'], g_obj['y'] + g_obj['h']]\n",
        "            iou = calculate_iou(p_bbox, g_bbox)\n",
        "            if iou > 0.1:\n",
        "                return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FUfmcvVHYtP_"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def calculate_iou(boxA, boxB):\n",
        "    xA = max(boxA[0], boxB[0]); yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2]); yB = min(boxA[3], boxB[3])\n",
        "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
        "    areaA = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
        "    areaB = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
        "    return interArea / float(areaA + areaB - interArea + 1e-6)\n",
        "\n",
        "def run_semantic_evaluation(root_dir, gt_path, threshold=0.60):\n",
        "    print(f\"Loading Ground Truth...\")\n",
        "    with open(gt_path, 'r') as f:\n",
        "        gt_data = json.load(f)\n",
        "\n",
        "    results = {\"total_items\": 0, \"semantic_hits\": 0, \"exact_hits\": 0}\n",
        "\n",
        "    print(\"Starting Semantic Analysis with Clothing Fix...\")\n",
        "    for root, dirs, files in os.walk(root_dir):\n",
        "        if \"data.json\" not in files: continue\n",
        "\n",
        "        with open(os.path.join(root, \"data.json\"), 'r') as f:\n",
        "            predictions = json.load(f)\n",
        "        if not predictions: continue\n",
        "\n",
        "        img_id = str(predictions[0].get('img_id'))\n",
        "        if img_id not in gt_data: continue\n",
        "        gt_objs = gt_data[img_id]['objects']\n",
        "\n",
        "        for p_obj in predictions:\n",
        "            p_bbox = p_obj.get('box')\n",
        "            p_label = p_obj.get('object_type', '').lower()\n",
        "            if not p_bbox: continue\n",
        "\n",
        "            best_iou = 0\n",
        "            best_gt_attrs = []\n",
        "            for g_id, g_obj in gt_objs.items():\n",
        "                g_bbox = [g_obj['x'], g_obj['y'], g_obj['x'] + g_obj['w'], g_obj['y'] + g_obj['h']]\n",
        "                iou = calculate_iou(p_bbox, g_bbox)\n",
        "                if iou > best_iou:\n",
        "                    best_iou = iou\n",
        "                    best_gt_attrs = [a.lower() for a in g_obj.get('attributes', [])]\n",
        "\n",
        "            if best_iou >= 0.3 and best_gt_attrs:\n",
        "                for cat, attrs in p_obj['attributes'].items():\n",
        "                    for attr in attrs:\n",
        "                        results[\"total_items\"] += 1\n",
        "                        is_hit = False\n",
        "\n",
        "                        if attr.lower() in best_gt_attrs:\n",
        "                            results[\"exact_hits\"] += 1\n",
        "                            results[\"semantic_hits\"] += 1\n",
        "                            is_hit = True\n",
        "\n",
        "                        if not is_hit and cat == \"clothing\" and p_label == \"person\":\n",
        "                            for g_id, g_obj in gt_objs.items():\n",
        "                                g_name = g_obj['name'].lower()\n",
        "                                if attr.lower() in g_name or g_name in attr.lower():\n",
        "                                    g_bbox = [g_obj['x'], g_obj['y'], g_obj['x'] + g_obj['w'], g_obj['y'] + g_obj['h']]\n",
        "                                    if calculate_iou(p_bbox, g_bbox) > 0.1:\n",
        "                                        results[\"semantic_hits\"] += 1\n",
        "                                        is_hit = True\n",
        "                                        break\n",
        "                        if not is_hit:\n",
        "                            pred_emb = sbert_model.encode([attr], convert_to_tensor=True)\n",
        "                            gt_embs = sbert_model.encode(best_gt_attrs, convert_to_tensor=True)\n",
        "                            cosine_scores = util.cos_sim(pred_emb, gt_embs)\n",
        "                            if torch.max(cosine_scores) >= threshold:\n",
        "                                results[\"semantic_hits\"] += 1\n",
        "                                is_hit = True\n",
        "\n",
        "    exact_acc = (results['exact_hits']/results['total_items'])*100 if results['total_items'] > 0 else 0\n",
        "    sem_acc = (results['semantic_hits']/results['total_items'])*100 if results['total_items'] > 0 else 0\n",
        "    print(f\"\\nExact Match: {exact_acc:.2f}% | Semantic Match: {sem_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jv_sif2EZYpw"
      },
      "outputs": [],
      "source": [
        "run_semantic_evaluation(drive_path, \"/content/drive/MyDrive/GQA_Research_2026_3/val_sceneGraphs.json\", 0.6)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyN0ozRn0D63FG076Y2yhagu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}