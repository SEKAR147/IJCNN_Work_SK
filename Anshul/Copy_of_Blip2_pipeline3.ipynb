{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNGrBrVVJO3iiWtRaVZcSiW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SEKAR147/IJCNN_Work_SK/blob/main/Copy_of_Blip2_pipeline3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MrMGb75HY20"
      },
      "outputs": [],
      "source": [
        "# Standard \"Stable\" environment for your GQA project\n",
        "!pip install -q huggingface_hub>=0.25.0 transformers>=4.45.0\n",
        "!pip install -q accelerate bitsandbytes timm\n",
        "import torch\n",
        "from PIL import Image\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "from transformers import (\n",
        "    AutoProcessor,\n",
        "    AutoModelForZeroShotObjectDetection,\n",
        "    Blip2Processor,\n",
        "    Blip2ForConditionalGeneration,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "dino_id = \"IDEA-Research/grounding-dino-tiny\"\n",
        "dino_processor = AutoProcessor.from_pretrained(dino_id)\n",
        "dino_model = AutoModelForZeroShotObjectDetection.from_pretrained(dino_id).to(device)\n",
        "\n",
        "blip_id = \"Salesforce/blip2-opt-2.7b\"\n",
        "blip_processor = Blip2Processor.from_pretrained(blip_id)\n",
        "blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    blip_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "iqTDYKmBHeFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets\n",
        "from datasets import load_dataset\n",
        "from itertools import islice"
      ],
      "metadata": {
        "id": "RK-6knP-ILeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"lemmatizer\"])"
      ],
      "metadata": {
        "id": "9zH762T_YiMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pipeline_streaming(image, text_prompt, question_id, image_id, base_drive_path):\n",
        "    q_dir = os.path.join(base_drive_path, question_id)\n",
        "    crops_dir = os.path.join(q_dir, \"crops\")\n",
        "    os.makedirs(crops_dir, exist_ok=True)\n",
        "\n",
        "    image = image.convert(\"RGB\")\n",
        "    image.save(os.path.join(q_dir, \"original.jpg\"))\n",
        "\n",
        "    COLOR_SET = {\n",
        "        \"red\", \"blue\", \"green\", \"yellow\", \"black\", \"white\", \"silver\", \"gold\", \"orange\",\n",
        "        \"pink\", \"purple\", \"brown\", \"gray\", \"grey\", \"tan\", \"turquoise\", \"beige\", \"maroon\",\n",
        "        \"navy\", \"teal\", \"azure\", \"bronze\", \"copper\", \"lavender\", \"violet\", \"blonde\",\n",
        "        \"light\", \"dark\", \"bright\", \"pale\", \"colorful\", \"multicolored\"\n",
        "    }\n",
        "\n",
        "    MATERIAL_SET = {\n",
        "        \"wood\", \"metal\", \"plastic\", \"glass\", \"brick\", \"denim\", \"leather\", \"cloth\", \"stone\",\n",
        "        \"concrete\", \"paper\", \"rubber\", \"wool\", \"silk\", \"velvet\", \"fabric\", \"cotton\", \"nylon\",\n",
        "        \"steel\", \"aluminum\", \"iron\", \"chrome\", \"porcelain\", \"ceramic\", \"tile\", \"marble\",\n",
        "        \"asphalt\", \"cardboard\", \"wicker\", \"vinyl\", \"suede\", \"fleece\"\n",
        "    }\n",
        "\n",
        "    TEXTURE_SET = {\n",
        "        \"smooth\", \"rough\", \"shiny\", \"metallic\", \"soft\", \"hard\", \"fuzzy\", \"clear\", \"dull\",\n",
        "        \"matte\", \"wet\", \"dry\", \"painted\", \"polished\", \"glossy\", \"checkered\", \"striped\",\n",
        "        \"dotted\", \"patterned\", \"cracked\", \"dirty\", \"clean\", \"worn\", \"new\", \"old\", \"fuzzy\",\n",
        "        \"hairy\", \"scratched\", \"wrinkled\", \"rusted\", \"transparent\", \"opaque\"\n",
        "    }\n",
        "\n",
        "    SHAPE_SET = {\n",
        "        \"round\", \"square\", \"rectangular\", \"triangular\", \"oval\", \"flat\", \"curved\", \"pointed\",\n",
        "        \"thick\", \"thin\", \"wide\", \"narrow\", \"large\", \"small\", \"tiny\", \"huge\", \"long\", \"short\",\n",
        "        \"tall\", \"curvy\", \"straight\", \"bent\", \"circular\", \"spherical\", \"conical\", \"cylindrical\"\n",
        "    }\n",
        "\n",
        "    CLOTHING_SET = {\n",
        "        \"shirt\", \"pants\", \"hat\", \"glasses\", \"jacket\", \"jeans\", \"dress\", \"shorts\", \"shoes\",\n",
        "        \"t-shirt\", \"sweater\", \"suit\", \"tie\", \"skirt\", \"boots\", \"sneakers\", \"socks\", \"gloves\",\n",
        "        \"scarf\", \"belt\", \"cap\", \"helmet\", \"uniform\", \"vest\", \"coat\", \"hoodie\"\n",
        "    }\n",
        "\n",
        "    # --- PHASE 1: GROUNDING DINO ---\n",
        "    inputs = dino_processor(images=image, text=text_prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = dino_model(**inputs)\n",
        "\n",
        "    results = dino_processor.post_process_grounded_object_detection(\n",
        "        outputs, inputs.input_ids,\n",
        "        threshold=0.35, text_threshold=0.25,\n",
        "        target_sizes=[image.size[::-1]]\n",
        "    )[0]\n",
        "\n",
        "    final_data = []\n",
        "\n",
        "    # --- PHASE 2 & 3: OBJECT LOOP ---\n",
        "    for i, (score, label, box) in enumerate(zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"])):\n",
        "        box = [round(b, 2) for b in box.tolist()]\n",
        "        crop = image.crop((box[0], box[1], box[2], box[3]))\n",
        "\n",
        "        if crop.size[0] < 40 or crop.size[1] < 40:\n",
        "            continue\n",
        "\n",
        "        crop_name = f\"obj_{i}_{label}.jpg\"\n",
        "        crop.save(os.path.join(crops_dir, crop_name))\n",
        "\n",
        "        extracted_attributes = []\n",
        "        actions = []\n",
        "        obj_id = f\"gqa_2026_{question_id}_obj_{i}\"\n",
        "\n",
        "        # --- PHASE 2: BLIP-2 ---\n",
        "        if label.lower() == \"person\":\n",
        "            vqa_prompt = f\"Question: Describe the clothing and current activity of this person. Answer: This person is wearing\"\n",
        "        else:\n",
        "            vqa_prompt = f\"Question: What are the specific visual properties of this {label}? Describe its color and material. Answer: This {label} is\"\n",
        "        blip_inputs = blip_processor(images=crop, text=vqa_prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            gen_ids = blip_model.generate(**blip_inputs, max_new_tokens=40, min_new_tokens=10, repetition_penalty=1.2)\n",
        "            desc = blip_processor.batch_decode(gen_ids, skip_special_tokens=True)[0].strip()\n",
        "            desc = desc.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "        # --- PHASE 3: SPACY ---\n",
        "        doc = nlp(desc.lower())\n",
        "        categorized_attr = {\n",
        "            \"color\": [],\n",
        "            \"material\": [],\n",
        "            \"texture\": [],\n",
        "            \"shape\": [],\n",
        "            \"action\": [],\n",
        "            \"clothing\": []\n",
        "        }\n",
        "\n",
        "        for token in doc:\n",
        "            t_text = token.text\n",
        "            # 1. Action/Pose: Capture Present Participle verbs (-ing)\n",
        "            if token.pos_ == \"VERB\" and token.tag_ == \"VBG\":\n",
        "                if t_text not in [\"is\", \"being\", \"having\", \"pose\", \"wearing\"]:\n",
        "                    categorized_attr[\"action\"].append(t_text)\n",
        "\n",
        "            # 2. Categorize based on GQA Sets\n",
        "            if t_text in COLOR_SET:\n",
        "                categorized_attr[\"color\"].append(t_text)\n",
        "            if t_text in MATERIAL_SET:\n",
        "                categorized_attr[\"material\"].append(t_text)\n",
        "            if t_text in TEXTURE_SET:\n",
        "                categorized_attr[\"texture\"].append(t_text)\n",
        "            if t_text in SHAPE_SET:\n",
        "                categorized_attr[\"shape\"].append(t_text)\n",
        "            if t_text in CLOTHING_SET:\n",
        "                categorized_attr[\"clothing\"].append(t_text)\n",
        "\n",
        "        categorized_attr = {k: list(set(v)) for k, v in categorized_attr.items()}\n",
        "\n",
        "        final_data.append({\n",
        "            \"question_id\": question_id,\n",
        "            \"img_id\": image_id,\n",
        "            \"obj_id\": obj_id,\n",
        "            \"box\": box,\n",
        "            \"object_type\": label,\n",
        "            \"attributes\": categorized_attr, # Labeled output\n",
        "            \"crop_path\": os.path.join(\"crops\", crop_name),\n",
        "            \"raw\": desc\n",
        "        })\n",
        "\n",
        "    # Save the JSON for this specific image inside its folder\n",
        "    with open(os.path.join(q_dir, \"data.json\"), \"w\") as f:\n",
        "        json.dump(final_data, f, indent=4)\n",
        "\n",
        "    # Cleanup memory\n",
        "    del results, inputs, outputs\n",
        "    return final_data"
      ],
      "metadata": {
        "id": "OsrhIU2vHg4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "!fusermount -u /content/drive\n",
        "\n",
        "!rm -rf /content/drive\n",
        "\n",
        "!mkdir /content/drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "drive_path = \"/content/drive/MyDrive/GQA_Research_2026_7\"\n",
        "os.makedirs(drive_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "NylmSHX6xXeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "import os\n",
        "from itertools import islice\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"lmms-lab/GQA\", \"val_balanced_instructions\", streaming=True, split=\"val\")\n",
        "\n",
        "image_dataset = load_dataset(\"lmms-lab/GQA\", \"val_balanced_images\", streaming=True, split=\"val\")\n",
        "\n",
        "print(\"Building image lookup table...\")\n",
        "image_lookup = {}\n",
        "for img_ex in islice(image_dataset, 5000):\n",
        "    image_lookup[img_ex['id']] = img_ex['image']\n",
        "\n",
        "all_final_results = []\n",
        "num_questions = 1000\n",
        "prompt = \"person . clothing . bag . chair . table . house . boat . door . animal . car . sign . bottle . cup . food . plate .\"\n",
        "\n",
        "print(f\"Starting stream for {num_questions} images...\")\n",
        "\n",
        "for i, example in enumerate(islice(dataset, num_questions)):\n",
        "    q_id = example['id']\n",
        "    img_id = example['imageId']\n",
        "\n",
        "    if img_id not in image_lookup:\n",
        "        print(f\"Skipping Q {q_id}: Image {img_id} not found in cache.\")\n",
        "        continue\n",
        "\n",
        "    img = image_lookup[img_id]\n",
        "\n",
        "    try:\n",
        "        image_results = run_pipeline_streaming(img, prompt, question_id = q_id, image_id=img_id, base_drive_path= drive_path)\n",
        "        all_final_results.extend(image_results)\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping image {i} due to error: {e}\")\n",
        "\n",
        "    if i % 50 == 0 and i > 0:\n",
        "        checkpoint_file = os.path.join(drive_path, \"results_checkpoint.json\")\n",
        "        with open(checkpoint_file, \"w\") as f:\n",
        "            json.dump(all_final_results, f)\n",
        "        print(f\"Checkpoint saved at question {i}\")\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "final_file = os.path.join(drive_path, \"final_gqa_results.json\")\n",
        "with open(final_file, \"w\") as f:\n",
        "    json.dump(all_final_results, f)\n",
        "\n",
        "print(f\"Done! All results are saved in {drive_path}\")"
      ],
      "metadata": {
        "id": "PHcu0EQyIMMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def calculate_iou(boxA, boxB):\n",
        "    xA = max(boxA[0], boxB[0]); yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2]); yB = min(boxA[3], boxB[3])\n",
        "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
        "    areaA = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
        "    areaB = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
        "    return interArea / float(areaA + areaB - interArea + 1e-6)\n",
        "\n",
        "def run_semantic_evaluation(root_dir, gt_path, threshold=0.50):\n",
        "    print(f\"Loading Ground Truth...\")\n",
        "    with open(gt_path, 'r') as f:\n",
        "        gt_data = json.load(f)\n",
        "\n",
        "    results = {\"total_items\": 0, \"semantic_hits\": 0, \"exact_hits\": 0}\n",
        "\n",
        "    print(\"Starting Semantic Analysis...\")\n",
        "    for root, dirs, files in os.walk(root_dir):\n",
        "        if \"data.json\" not in files: continue\n",
        "\n",
        "        with open(os.path.join(root, \"data.json\"), 'r') as f:\n",
        "            predictions = json.load(f)\n",
        "        if not predictions: continue\n",
        "\n",
        "        img_id = str(predictions[0].get('img_id'))\n",
        "        if img_id not in gt_data: continue\n",
        "        gt_objs = gt_data[img_id]['objects']\n",
        "\n",
        "        for p_obj in predictions:\n",
        "            p_bbox = p_obj.get('box')\n",
        "            if not p_bbox: continue\n",
        "\n",
        "            # --- SPATIAL MATCHING ---\n",
        "            best_iou = 0\n",
        "            best_gt_attrs = []\n",
        "\n",
        "            for g_id, g_obj in gt_objs.items():\n",
        "                # GQA [x, y, w, h] -> [x1, y1, x2, y2]\n",
        "                g_bbox = [g_obj['x'], g_obj['y'], g_obj['x'] + g_obj['w'], g_obj['y'] + g_obj['h']]\n",
        "                iou = calculate_iou(p_bbox, g_bbox)\n",
        "                if iou > best_iou:\n",
        "                    best_iou = iou\n",
        "                    best_gt_attrs = [a.lower() for a in g_obj.get('attributes', [])]\n",
        "\n",
        "\n",
        "            # Only proceed if we actually matched an object (IoU > 0.5)\n",
        "            if best_iou >= 0.5 and best_gt_attrs:\n",
        "                pred_list = [a for sublist in p_obj['attributes'].values() for a in sublist]\n",
        "                if not pred_list: continue\n",
        "\n",
        "                # --- SEMANTIC MATCHING ---\n",
        "                # 1. Check Exact Matches first (Fast)\n",
        "                exact_matches = [p for p in pred_list if p.lower() in best_gt_attrs]\n",
        "                results[\"exact_hits\"] += len(exact_matches)\n",
        "\n",
        "                # 2. Check remaining for Semantic Similarity\n",
        "                remaining_preds = [p for p in pred_list if p.lower() not in best_gt_attrs]\n",
        "                results[\"total_items\"] += len(pred_list)\n",
        "\n",
        "                if remaining_preds:\n",
        "                    pred_embeddings = sbert_model.encode(remaining_preds, convert_to_tensor=True)\n",
        "                    gt_embeddings = sbert_model.encode(best_gt_attrs, convert_to_tensor=True)\n",
        "                    cosine_scores = util.cos_sim(pred_embeddings, gt_embeddings)\n",
        "\n",
        "                    for i in range(len(remaining_preds)):\n",
        "                        if torch.max(cosine_scores[i]) >= threshold:\n",
        "                            results[\"semantic_hits\"] += 1\n",
        "\n",
        "                # Add exact hits to semantic total\n",
        "                results[\"semantic_hits\"] += len(exact_matches)\n",
        "\n",
        "    # Final Report\n",
        "    exact_acc = (results['exact_hits']/results['total_items'])*100 if results['total_items'] > 0 else 0\n",
        "    sem_acc = (results['semantic_hits']/results['total_items'])*100 if results['total_items'] > 0 else 0\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"      SEMANTIC EVALUATION REPORT\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"Exact Match Accuracy:   {exact_acc:.2f}%\")\n",
        "    print(f\"Semantic Match Accuracy: {sem_acc:.2f}% (Threshold: {threshold})\")\n",
        "    print(f\"Total Attributes Scored: {results['total_items']}\")\n",
        "    print(\"=\"*40)\n"
      ],
      "metadata": {
        "id": "FUfmcvVHYtP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_semantic_evaluation(drive_path, \"/content/drive/MyDrive/GQA_Research_2026_3/val_sceneGraphs.json\", 0.6)"
      ],
      "metadata": {
        "id": "Jv_sif2EZYpw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}